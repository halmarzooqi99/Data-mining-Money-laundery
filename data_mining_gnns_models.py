# -*- coding: utf-8 -*-
"""Data mining gnns-models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d9ChoqVntNPl7RVfzQ-_7XKc5pI6cEOA
"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d trnaacthng/multi-gnn

!unzip -q multi-gnn.zip -d multi_gnn_data

!pip install torch_geometric

"""**GAT Model**"""

import os
import torch
import pandas as pd
import numpy as np
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from imblearn.under_sampling import RandomUnderSampler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.cuda.amp import autocast, GradScaler
!pip install torch_geometric torch_sparse torch_scatter torch_spline_conv torch_cluster -f https://data.pyg.org/whl/torch-2.1.0+cu118.html

# PyTorch Geometric imports
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
from sklearn.neighbors import NearestNeighbors

# Check GPU availability
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# --------------------------
# Data Preprocessing
# --------------------------
data = pd.read_csv("/content/HI-Small_Trans.csv")
data.drop(columns=["Timestamp", "Receiving Currency", "Payment Currency"], inplace=True)

# Separate features and labels
X = data.drop(columns=["Is Laundering"])
y = data["Is Laundering"]

# Encode categorical columns
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
for col in categorical_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])

# Stratified train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.1, random_state=42, stratify=y)

# Handle class imbalance using undersampling (or try oversampling)
sampler = RandomUnderSampler(random_state=42)
X_train, y_train = sampler.fit_resample(X_train, y_train)
X_test, y_test = sampler.fit_resample(X_test, y_test)

# Standardize numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Convert to tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
X_test_tensor  = torch.tensor(X_test, dtype=torch.float32).to(device)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.long).to(device)
y_test_tensor  = torch.tensor(y_test.values, dtype=torch.long).to(device)

# --------------------------
# Build Graph Structure via kNN with Self-loops
# --------------------------
def create_knn_edge_index(features, k=20):
    nbrs = NearestNeighbors(n_neighbors=k, metric='euclidean').fit(features)
    distances, indices = nbrs.kneighbors(features)
    edge_list = []
    num_nodes = features.shape[0]
    for i in range(num_nodes):
        # Add self-loop for node i
        edge_list.append([i, i])
        for j in indices[i]:
            if i != j:
                edge_list.append([i, j])
    edge_index = np.array(edge_list).T  # shape [2, num_edges]
    return torch.tensor(edge_index, dtype=torch.long).to(device)

edge_index_train = create_knn_edge_index(X_train, k=20)
edge_index_test  = create_knn_edge_index(X_test, k=20)

# Create PyG Data objects for training and testing
train_data = Data(x=X_train_tensor, edge_index=edge_index_train, y=y_train_tensor)
test_data  = Data(x=X_test_tensor, edge_index=edge_index_test, y=y_test_tensor)

# --------------------------
# Define Deep Optimized GAT Model
# --------------------------
class DeepOptimizedGAT(nn.Module):
    def __init__(self, input_dim, heads=16):
        super(DeepOptimizedGAT, self).__init__()
        # First GAT layer: from input_dim to 16 features per head (concatenated output)
        self.gat1 = GATConv(input_dim, 16, heads=heads, dropout=0.3)
        self.bn1 = nn.BatchNorm1d(16 * heads)

        # Second GAT layer: from (16*heads) to 32 features per head
        self.gat2 = GATConv(16 * heads, 32, heads=heads, dropout=0.3)
        self.bn2 = nn.BatchNorm1d(32 * heads)

        # Third GAT layer: from (32*heads) to 32 features per head
        self.gat3 = GATConv(32 * heads, 32, heads=heads, dropout=0.3)
        self.bn3 = nn.BatchNorm1d(32 * heads)

        # Fourth GAT layer: use a single head with no concatenation
        self.gat4 = GATConv(32 * heads, 32, heads=1, concat=False, dropout=0.3)
        self.bn4 = nn.BatchNorm1d(32)

        # Final linear layer for classification
        self.fc = nn.Linear(32, 2)
        self.dropout = nn.Dropout(0.3)

    def forward(self, x, edge_index):
        x = self.gat1(x, edge_index)
        x = self.bn1(x)
        x = F.elu(x)
        x = self.dropout(x)

        x = self.gat2(x, edge_index)
        x = self.bn2(x)
        x = F.elu(x)
        x = self.dropout(x)

        x = self.gat3(x, edge_index)
        x = self.bn3(x)
        x = F.elu(x)
        x = self.dropout(x)

        x = self.gat4(x, edge_index)
        x = self.bn4(x)
        x = F.elu(x)
        x = self.dropout(x)

        out = self.fc(x)
        return out

model = DeepOptimizedGAT(input_dim=X_train.shape[1], heads=16).to(device)
optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)
criterion = nn.CrossEntropyLoss()
scaler = GradScaler()

# --------------------------
# Training Function with Early Stopping and Metric Logging
# --------------------------
def train_model(model, train_data, test_data, optimizer, scheduler, criterion, epochs=100, patience=10):
    best_loss = float('inf')
    early_stop_counter = 0

    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        with autocast():
            out = model(train_data.x, train_data.edge_index)
            loss = criterion(out, train_data.y)
        scaler.scale(loss).backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        scaler.step(optimizer)
        scaler.update()

        train_acc = (out.argmax(dim=1) == train_data.y).sum().item() / train_data.y.size(0)

        model.eval()
        with torch.no_grad():
            out_val = model(test_data.x, test_data.edge_index)
            val_loss = criterion(out_val, test_data.y)
            val_acc = (out_val.argmax(dim=1) == test_data.y).sum().item() / test_data.y.size(0)

        scheduler.step(val_loss)

        train_losses.append(loss.item())
        val_losses.append(val_loss.item())
        train_accs.append(train_acc)
        val_accs.append(val_acc)

        print(f"Epoch {epoch+1}: Loss: {loss.item():.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss.item():.4f}, Val Acc: {val_acc:.4f}")

        if val_loss.item() < best_loss:
            best_loss = val_loss.item()
            early_stop_counter = 0
            torch.save(model.state_dict(), "best_deep_gat_model.pth")
        else:
            early_stop_counter += 1
            if early_stop_counter >= patience:
                print("Early stopping triggered!")
                break

    return train_losses, val_losses, train_accs, val_accs

train_losses, val_losses, train_accs, val_accs = train_model(
    model, train_data, test_data, optimizer, scheduler, criterion, epochs=100, patience=10
)

# --------------------------
# Plot Loss and Accuracy Curves
# --------------------------
epochs_range = range(1, len(train_losses)+1)

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, train_losses, label='Train Loss')
plt.plot(epochs_range, val_losses, label='Validation Loss')
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Loss Curve")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs_range, train_accs, label='Train Accuracy')
plt.plot(epochs_range, val_accs, label='Validation Accuracy')
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Accuracy Curve")
plt.legend()

plt.tight_layout()
plt.show()

# --------------------------
# Evaluation and Plotting Confusion Matrix & Metrics
# --------------------------
# Load the best model and evaluate
model.load_state_dict(torch.load("best_deep_gat_model.pth"))
model.eval()
with torch.no_grad():
    out_test = model(test_data.x, test_data.edge_index)
    y_pred = out_test.argmax(dim=1).cpu().numpy()
    y_true = test_data.y.cpu().numpy()

# Print classification report
print("Classification Report:\n", classification_report(y_true, y_pred))
print("Accuracy: ", accuracy_score(y_true, y_pred))

# Plot Confusion Matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

# Compute evaluation metrics and plot as a bar chart
precision = precision_score(y_true, y_pred, average='macro')
recall = recall_score(y_true, y_pred, average='macro')
f1 = f1_score(y_true, y_pred, average='macro')

metrics = {"Precision": precision, "Recall": recall, "F1 Score": f1}
plt.figure(figsize=(6, 5))
sns.barplot(x=list(metrics.keys()), y=list(metrics.values()))
plt.ylim(0, 1)
plt.title("Evaluation Metrics")
plt.show()

"""# New Section

# New Section

**GCN Model**
"""

import os
import torch
import pandas as pd
import numpy as np
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from imblearn.under_sampling import RandomUnderSampler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.cuda.amp import autocast, GradScaler

# PyTorch Geometric imports
!pip install torch_geometric torch_sparse torch_scatter torch_spline_conv torch_cluster -f https://data.pyg.org/whl/torch-2.1.0+cu118.html
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
from sklearn.neighbors import NearestNeighbors

# Check GPU availability
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# --------------------------
# Data Preprocessing
# --------------------------
data = pd.read_csv("/content/HI-Small_Trans.csv")
data.drop(columns=["Timestamp","Receiving Currency", "Payment Currency"], inplace=True)

# Separate features and labels
X = data.drop(columns=["Is Laundering"])
y = data["Is Laundering"]

# Encode categorical columns
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
for col in categorical_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])

# Stratified train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.1, random_state=42, stratify=y)

# Handle class imbalance (using undersampling, or try oversampling as needed)
sampler = RandomUnderSampler(random_state=42)
X_train, y_train = sampler.fit_resample(X_train, y_train)
X_test, y_test   = sampler.fit_resample(X_test, y_test)

# Standardize numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test  = scaler.transform(X_test)

# Convert to tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
X_test_tensor  = torch.tensor(X_test, dtype=torch.float32).to(device)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.long).to(device)
y_test_tensor  = torch.tensor(y_test.values, dtype=torch.long).to(device)

# --------------------------
# Build Graph Structure via kNN with Self-loops
# --------------------------
def create_knn_edge_index(features, k=20):
    nbrs = NearestNeighbors(n_neighbors=k, metric='euclidean').fit(features)
    distances, indices = nbrs.kneighbors(features)
    edge_list = []
    num_nodes = features.shape[0]
    for i in range(num_nodes):
        # Add self-loop
        edge_list.append([i, i])
        for j in indices[i]:
            if i != j:
                edge_list.append([i, j])
    edge_index = np.array(edge_list).T  # shape [2, num_edges]
    return torch.tensor(edge_index, dtype=torch.long).to(device)

edge_index_train = create_knn_edge_index(X_train, k=20)
edge_index_test  = create_knn_edge_index(X_test, k=20)

# Create PyG Data objects
train_data = Data(x=X_train_tensor, edge_index=edge_index_train, y=y_train_tensor)
test_data  = Data(x=X_test_tensor, edge_index=edge_index_test, y=y_test_tensor)

# --------------------------
# Define Deep Optimized GCN Model
# --------------------------
class DeepOptimizedGCN(nn.Module):
    def __init__(self, input_dim):
        super(DeepOptimizedGCN, self).__init__()
        # First GCN layer: input -> 128 features
        self.conv1 = GCNConv(input_dim, 128)
        self.bn1   = nn.BatchNorm1d(128)
        # Second GCN layer: 128 -> 64 features
        self.conv2 = GCNConv(128, 64)
        self.bn2   = nn.BatchNorm1d(64)
        # Third GCN layer: 64 -> 64 features
        self.conv3 = GCNConv(64, 64)
        self.bn3   = nn.BatchNorm1d(64)
        # Fourth GCN layer: 64 -> 32 features
        self.conv4 = GCNConv(64, 32)
        self.bn4   = nn.BatchNorm1d(32)
        # Final classification layer
        self.fc = nn.Linear(32, 2)
        self.dropout = nn.Dropout(0.3)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.dropout(x)

        x = self.conv2(x, edge_index)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.dropout(x)

        x = self.conv3(x, edge_index)
        x = self.bn3(x)
        x = F.relu(x)
        x = self.dropout(x)

        x = self.conv4(x, edge_index)
        x = self.bn4(x)
        x = F.relu(x)
        x = self.dropout(x)

        out = self.fc(x)
        return out

model = DeepOptimizedGCN(input_dim=X_train.shape[1]).to(device)
optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)
criterion = nn.CrossEntropyLoss()
scaler = GradScaler()

# --------------------------
# Training Function with Early Stopping and Metric Logging
# --------------------------
def train_model(model, train_data, test_data, optimizer, scheduler, criterion, epochs=100, patience=10):
    best_loss = float('inf')
    early_stop_counter = 0

    train_losses = []
    val_losses   = []
    train_accs   = []
    val_accs     = []

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        with autocast():
            out = model(train_data.x, train_data.edge_index)
            loss = criterion(out, train_data.y)
        scaler.scale(loss).backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        scaler.step(optimizer)
        scaler.update()

        train_acc = (out.argmax(dim=1) == train_data.y).sum().item() / train_data.y.size(0)

        model.eval()
        with torch.no_grad():
            out_val = model(test_data.x, test_data.edge_index)
            val_loss = criterion(out_val, test_data.y)
            val_acc = (out_val.argmax(dim=1) == test_data.y).sum().item() / test_data.y.size(0)

        scheduler.step(val_loss)

        train_losses.append(loss.item())
        val_losses.append(val_loss.item())
        train_accs.append(train_acc)
        val_accs.append(val_acc)

        print(f"Epoch {epoch+1}: Loss: {loss.item():.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss.item():.4f}, Val Acc: {val_acc:.4f}")

        if val_loss.item() < best_loss:
            best_loss = val_loss.item()
            early_stop_counter = 0
            torch.save(model.state_dict(), "best_deep_gcn_model.pth")
        else:
            early_stop_counter += 1
            if early_stop_counter >= patience:
                print("Early stopping triggered!")
                break

    return train_losses, val_losses, train_accs, val_accs

train_losses, val_losses, train_accs, val_accs = train_model(
    model, train_data, test_data, optimizer, scheduler, criterion, epochs=500, patience=10
)

# --------------------------
# Plot Loss and Accuracy Curves
# --------------------------
epochs_range = range(1, len(train_losses) + 1)

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, train_losses, label='Train Loss')
plt.plot(epochs_range, val_losses, label='Validation Loss')
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Loss Curve")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs_range, train_accs, label='Train Accuracy')
plt.plot(epochs_range, val_accs, label='Validation Accuracy')
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Accuracy Curve")
plt.legend()

plt.tight_layout()
plt.show()

# --------------------------
# Evaluation and Plotting Confusion Matrix & Metrics
# --------------------------
# Load best model and evaluate on test data
model.load_state_dict(torch.load("best_deep_gcn_model.pth"))
model.eval()
with torch.no_grad():
    out_test = model(test_data.x, test_data.edge_index)
    y_pred = out_test.argmax(dim=1).cpu().numpy()
    y_true = test_data.y.cpu().numpy()

# Print classification report and overall accuracy
print("Classification Report:\n", classification_report(y_true, y_pred))
print("Accuracy: ", accuracy_score(y_true, y_pred))

# Plot Confusion Matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

# Compute evaluation metrics and plot as a bar chart
precision = precision_score(y_true, y_pred, average='macro')
recall = recall_score(y_true, y_pred, average='macro')
f1 = f1_score(y_true, y_pred, average='macro')

metrics = {"Precision": precision, "Recall": recall, "F1 Score": f1}
plt.figure(figsize=(6, 5))
sns.barplot(x=list(metrics.keys()), y=list(metrics.values()))
plt.ylim(0, 1)
plt.title("Evaluation Metrics")
plt.show()